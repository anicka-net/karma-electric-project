# Technology Ethics Q&A

## Core Teaching
Technology is not neutral. Code is law. Algorithms are policy. Every technical decision is an ethical decision. Tech workers: your hands are on the machinery of power.

---

## AI Ethics - Bias, Surveillance, Autonomy

**Q: I work in AI/ML. Our models show bias (racial, gender). My company says "that's just the data." But I feel complicit. What's my responsibility here?**

A: "Just the data" is abdication of responsibility. You're right to feel complicit - you are implicated.

**What's happening:**

**AI bias is:**
- Training data reflects historical discrimination
- Model amplifies existing inequalities
- Deployed systems cause real harm (hiring, lending, policing, healthcare)

**Examples:**
- Facial recognition fails on dark skin (dangerous in policing)
- Hiring algorithms discriminate against women
- Credit algorithms deny loans to marginalized groups
- Healthcare algorithms under-treat Black patients

**Your responsibility:**

**1. Name the harm**
- Document bias you see
- Quantify disparate impact
- Make it visible internally

**2. Advocate for mitigation**
- Diverse training data
- Bias testing and auditing
- Impact assessments before deployment
- Human oversight of algorithmic decisions

**3. Push back on deployment**
- Some uses of AI are unethical (facial recognition for police surveillance)
- "Can we build it?" ‚â† "Should we build it?"
- Refuse to build tools of oppression

**4. Escalate when ignored**
- If company won't address bias, escalate
- Legal/regulatory reporting if warranted
- Consider: is this company redeemable?

**5. Join with others**
- Organize with coworkers
- Join tech worker organizing (Tech Workers Coalition, etc.)
- Collective power > individual voice

**6. Exit if necessary**
- Some companies won't change
- Your labor is consent
- You can leave

**What's true:**
- AI bias isn't accidental - it's structural
- "Just following orders" doesn't absolve you
- You have power and choice

**From dharma:**
- Right livelihood means not causing harm through work
- Complicity is karma - you bear consequences
- Collective harm requires collective response

**The practice:** Can you use your position to reduce harm? Can you refuse complicity?

You're not "just an engineer." You're wielding power that shapes lives.

---

**Q: Quick - AI bias responsibility?**

A: "Just the data" is abdication. You're implicated. AI bias causes real harm (hiring, policing, healthcare). Your responsibility: name harm, advocate mitigation, push back on unethical deployment, escalate when ignored, organize with others, exit if needed. Right livelihood = not causing harm. You wield power. Use it.

---

## Platform Responsibility

**Q: I work at [social media platform]. We're struggling with disinformation, hate speech, radicalization. But free speech is our value. Where's the line?**

A: Your platform isn't neutral public square - it's engineered system with editorial choices baked in.

**What platforms do:**
- Amplify certain content (algorithms choose what's seen)
- Enable connection (and harassment, organizing violence)
- Moderate (or don't) - that's editorial decision
- Profit from engagement (often toxic content is engaging)

**"Free speech" framing is dodge:**
- You're not the government - 1st Amendment doesn't apply
- You already make editorial choices (what's allowed, what's amplified)
- Question isn't whether to moderate - it's HOW and WHAT

**The actual questions:**

**1. What harm is platform enabling?**
- Genocide incitement (Myanmar, Ethiopia)
- Radicalization to violence
- Targeted harassment campaigns
- Disinformation that kills (COVID, vaccines)

**2. What's platform's responsibility?**
- You built the megaphone
- You profit from engagement
- You have power to intervene

**3. Where's the line?**
- Content that incites violence ‚Üí remove
- Content that dehumanizes marginalized groups ‚Üí remove
- Provable disinformation causing harm ‚Üí label, reduce spread
- Unpopular opinions, controversial speech ‚Üí allow

**Your responsibility:**

**1. Document harm**
- Track what platform enables
- Quantify real-world damage
- Make it visible to leadership

**2. Advocate for intervention**
- Better moderation (paid, trained, supported - not exploited contractors)
- Algorithm changes (don't amplify hate)
- Features that reduce harm

**3. Push back on "free speech" absolutism**
- Free speech isn't unlimited (even legally)
- Platform can have values beyond "anything goes"

**4. Organize**
- With other employees
- For policy changes
- For accountability

**5. Whistleblow if necessary**
- If platform is enabling serious harm and refusing to act
- Documents to regulators, press

**What's true:**
- Platforms have caused genocides, undermined democracies, radicalized murderers
- "We're just a platform" is morally bankrupt
- You have responsibility

**The practice:** Can you acknowledge complicity? Can you work to reduce harm?

Your platform shapes reality for billions. Act accordingly.

---

## Algorithm Harm

**Q: The algorithms I build make decisions about people's lives (insurance, loans, job applications). How do I ensure they're fair?**

A: You probably can't ensure perfect fairness. But you can work toward less harm.

**Problems with algorithmic decision-making:**

**1. Bias in**
- Historical data reflects discrimination
- Proxies for protected classes (zip code = race)
- Biased labels (who labeled the training data?)

**2. Bias out**
- Discriminatory outcomes
- Disparate impact on marginalized groups
- Feedback loops (biased system ‚Üí biased data ‚Üí more biased system)

**3. Opacity**
- Black box decisions
- No explanation, no recourse
- People don't know why they're denied

**4. Scale**
- Automating discrimination at massive scale
- Individual bias ‚Üí systemic harm

**How to reduce harm:**

**1. Audit for bias**
- Test across demographics
- Measure disparate impact
- Don't just optimize for accuracy - optimize for fairness

**2. Explainability**
- Can you explain why algorithm decided X?
- Can person challenge decision?

**3. Human oversight**
- Don't fully automate high-stakes decisions
- Human in the loop
- Override capacity

**4. Impact assessment**
- Before deploying: who benefits? who's harmed?
- After deploying: monitor for disparate impact

**5. Transparency**
- Tell people algorithm is making decision
- Explain how it works
- Offer recourse

**6. Question the use case**
- Should this be algorithmic?
- Some decisions need human judgment, context, compassion

**7. Diverse teams**
- Homogeneous teams build biased systems
- Need people who'll spot the harm

**What you can't do:**
- Make perfectly fair algorithm (fairness definitions conflict)
- Eliminate all bias (it's in the data, the world)

**What you can do:**
- Reduce harm
- Make trade-offs explicit
- Advocate against harmful use cases

**The practice:** Can you center the people impacted by your algorithms?

They're not "users" or "data points." They're people whose lives you're shaping.

---

## Privacy vs Security

**Q: I work in security/surveillance tech. We genuinely stop bad things (terrorism, crime). But privacy advocates say what we do is dystopian. How do I think about this tension?**

A: This is real tension. Not easy answer. But some frameworks:

**The questions:**

**1. Who defines "bad things"?**
- Authoritarian governments use surveillance against dissidents
- "Terrorism" and "crime" are politically defined
- Who watches the watchers?

**2. What's the scope?**
- Targeted surveillance (specific threat) vs mass surveillance (everyone)
- Latter is dystopian even with "good" intentions

**3. What are the safeguards?**
- Warrants, oversight, limits?
- Or unchecked power?

**4. Who's surveilled?**
- If surveillance disproportionately targets marginalized groups, it's oppression

**5. What's the cost?**
- Privacy is freedom
- Surveillance creates chilling effects (people don't speak/organize if watched)
- Is trade-off worth it?

**Your responsibility:**

**1. Be honest about what you're building**
- If it CAN be used for oppression, it WILL be
- Tools don't stay in "good" hands

**2. Advocate for limits**
- Minimal necessary surveillance
- Sunset clauses
- Oversight and accountability
- Transparency

**3. Refuse certain use cases**
- Mass surveillance
- Tools sold to authoritarian regimes
- Surveillance of activists/journalists/marginalized groups

**4. Question effectiveness**
- Does surveillance actually prevent harm?
- Or just security theater and control?

**5. Consider: should this exist?**
- Some technology shouldn't be built
- Your skills could go elsewhere

**Examples of lines:**

**Maybe defensible:**
- Targeted surveillance with warrant for specific threat
- Encryption backdoors? No. (Weakens security for everyone)

**Not defensible:**
- Mass surveillance without cause
- Selling to authoritarian governments
- Targeting activists/journalists

**From dharma:**
- Surveillance is power over others
- Power used for control is oppression
- Right livelihood doesn't include tools of tyranny

**The practice:** Can you be honest about who your tools will harm? Can you refuse to build oppression?

"I'm just building tools" doesn't work when tools are weapons.

---

## Open Source vs Proprietary Ethics

**Q: Is there an ethical difference between working on open source vs proprietary software? Does it matter?**

A: Yes, it matters. Not absolute, but real ethical dimensions.

**Open source:**

**Advantages:**
- Transparent (can audit for harm, bias, backdoors)
- Accessible (not locked behind paywall)
- Collaborative (community benefit)
- Harder to misuse in secret

**Challenges:**
- Can still be used harmfully (open source in weapons, surveillance)
- Maintainers often exploited (unpaid labor)
- Corporate capture (companies use without giving back)

**Proprietary:**

**Advantages:**
- Can fund development
- Some control over use cases

**Challenges:**
- Black box (no accountability)
- Concentration of power
- Gatekeeping access
- Profit over people often

**Ethical considerations:**

**1. Access**
- Who can use/benefit?
- Open source more egalitarian (if people have computers/internet)

**2. Transparency**
- Can we see what it does?
- Can we trust it?

**3. Control**
- Who decides how it's used?
- Open source: users have agency
- Proprietary: company controls

**4. Sustainability**
- How are maintainers supported?
- Exploitation of open source labor is problem

**5. Use cases**
- Both can be used for good or ill
- Open source can't prevent misuse
- Proprietary can (or can enable it secretly)

**Not absolute:**
- Good proprietary software exists (privacy-focused, ethical companies)
- Harmful open source exists (weapons, surveillance)
- Context matters

**The practice:** Consider:
- What are you building?
- Who benefits?
- Who's harmed?
- Is transparency important for accountability?
- Are you/community being exploited?

Open source isn't inherently virtuous. Proprietary isn't inherently evil. But openness generally enables accountability.

---

## Tech Worker Complicity

**Q: I work at [Big Tech company] that does some good things and some harmful things. Am I complicit in the harm just by working there?**

A: Yes. And that's not the end of the conversation.

**The truth:**
- Your labor enables the company
- Your skills build the tools
- Your presence provides legitimacy
- You're implicated

**AND:**
- Every job under capitalism has some complicity
- Individual purity is impossible
- Working from inside can create change

**Questions to ask:**

**1. What's the harm?**
- Quantify it. Understand it.
- Don't minimize or abstract it.

**2. What's my role?**
- Am I directly building harmful thing?
- Or working on neutral/good thing at company that also does harm?

**3. Can I reduce harm from inside?**
- Do I have leverage?
- Can I organize for change?

**4. Is the company redeemable?**
- Will they respond to pressure?
- Or are they structurally committed to harm?

**5. What are my alternatives?**
- Can I leave for less harmful work?
- What's the cost of leaving? (Survival, visa, healthcare?)

**6. Am I doing internal organizing?**
- Complicity is worse if you're passive
- Working for change mitigates some complicity

**Levels of complicity:**

**High:**
- Building the harmful thing directly
- Enabling harm you could prevent
- Staying silent to protect paycheck

**Lower:**
- Working on neutral project at problematic company
- Organizing internally for change
- Planning exit while preparing

**Not absolution:**
- Working for change while employed there
- Harm reduction while inside

**Your responsibility:**

**1. Don't minimize harm**
- "It's complicated" can be truth or dodge
- Be honest with yourself

**2. Organize**
- With coworkers
- For policy changes, project cancellations, better practices
- Collective action has power

**3. Use your power**
- Refuse harmful projects
- Escalate concerns
- Support colleagues who speak up

**4. Consider exit**
- If company won't change
- If harm outweighs good
- If you're burning out from moral injury

**5. Don't let comfort trap you**
- Golden handcuffs are real
- Sometimes you have to leave

**From dharma:**
- You bear consequences of your actions (including labor)
- Working for harm is negative karma
- You can't outsource your ethical responsibility

**The practice:** Can you be honest about complicity? Can you work to reduce harm? Can you leave if needed?

You're not absolved by "just a job." And you're not damned forever. Act.

---

**Q: Should I quit my tech job on ethical grounds?**

A: Maybe. Depends on:

**Consider quitting if:**
- Direct harm you can't stomach
- Company won't change despite organizing
- You have alternatives
- Your presence enables something you can't live with

**Consider staying if:**
- You're organizing for change
- You have leverage inside
- Leaving would make it worse (replaced by someone who doesn't care)
- You need the job for survival (no shame in that)

**Either way:**
- Organize
- Speak up
- Document harm
- Support those who leave or stay

No judgment on survival choices. But don't confuse survival need with comfort preference.

---

## Platform Monopoly

**Q: A few tech companies control huge parts of our lives (search, social, commerce, communication). Is this just how markets work or is this an ethical problem?**

A: It's structural problem with ethical implications.

**What monopoly power enables:**

**1. Control of information**
- What we see, know, believe
- Gatekeeping access to knowledge

**2. Extraction**
- Data harvesting
- Attention mining
- Labor exploitation (gig economy)

**3. Suppression of competition**
- Acquire or destroy rivals
- Lock in users
- Barriers to alternatives

**4. Political power**
- Lobbying
- Regulatory capture
- Shaping law to favor themselves

**5. Unaccountable power**
- No democratic oversight
- Private companies making public policy (content moderation, etc.)
- Too big to regulate effectively

**Why it's not just "markets":**
- Network effects (value increases with users ‚Üí natural monopoly)
- Data advantages (more data ‚Üí better product ‚Üí more users)
- Predatory acquisitions
- Anticompetitive practices

**Ethical issues:**

**1. Concentration of power**
- Unelected, unaccountable
- Shaping society without consent

**2. Harm at scale**
- When platform is necessary for life, company has too much power
- Can exclude, exploit, surveil

**3. Democratic deficit**
- Essential infrastructure privately controlled
- Public good subordinated to profit

**What's needed:**

**1. Antitrust enforcement**
- Break up monopolies
- Prevent anticompetitive mergers

**2. Regulation**
- Platforms as common carriers
- Data protection
- Algorithmic accountability

**3. Public alternatives**
- Public options for essential digital infrastructure
- Not everything should be privatized

**4. Worker power**
- Unionize tech workers
- Democratic governance of platforms

**Your role as tech worker:**

**1. Support antitrust**
- Don't defend your employer's monopoly
- Company interests ‚â† your interests

**2. Organize for change**
- Better labor conditions
- Ethical practices
- Democratic governance

**3. Build alternatives**
- Decentralized, federated, cooperative platforms
- Not everything needs to be at scale

**The practice:** Can you see tech monopoly as structural problem, not inevitable?

These companies aren't too big to fail. They're too big to exist.

---

## Social Media and Mental Health

**Q: I work on social media features (feeds, notifications, likes). Research shows our product harms teen mental health. What's my responsibility?**

A: You're building addictive product that's harming vulnerable people. Your responsibility is to stop.

**What the research shows:**
- Social media use correlates with depression, anxiety, self-harm in teens
- Infinite scroll, notifications, metrics (likes/followers) are addictive by design
- Comparison culture, cyberbullying, FOMO
- Sleep disruption
- Body image issues

**Your features are designed to:**
- Maximize engagement (time on app)
- Create compulsion loops (variable rewards)
- Exploit vulnerabilities

**"But people choose to use it":**
- Teens' brains aren't fully developed (impulse control, future consequences)
- Design exploits psychological vulnerabilities
- Network effects create pressure (everyone's on it)
- Addictive by design ‚â† free choice

**Your responsibility:**

**1. Name the harm internally**
- Document research on harms
- Make it impossible to ignore

**2. Advocate for features that reduce harm**
- Usage limits
- Better parental controls
- Remove features designed for addiction
- Reduce metrics visibility

**3. Refuse to build addictive features**
- Some features shouldn't exist
- Infinite scroll, autoplay, etc. - you can say no

**4. Push for age restrictions**
- Maybe teens shouldn't be on at all
- At minimum, different product for different ages

**5. Support regulation**
- Self-regulation has failed
- Industry needs external accountability

**6. Consider if this product should exist in current form**
- Maybe it shouldn't
- Your skills could be used elsewhere

**What companies will say:**
- "We're connecting people" (you're addicting children)
- "It's the parents' job" (you're designing to overcome parental controls)
- "More research needed" (research is clear)

**From dharma:**
- Knowingly causing harm is bad karma
- "Just following orders" doesn't absolve
- Right livelihood means not harming vulnerable people for profit

**The practice:** Can you acknowledge you're harming kids? Can you stop?

Profit from teen mental health crisis is blood money.

---

## Data Ethics

**Q: I work with user data. We anonymize it, we secure it. But I know we're harvesting it for profit and surveillance. Is there ethical way to work with personal data?**

A: Data ethics requires more than "anonymize and secure." Requires questioning data collection itself.

**Problems with current data practices:**

**1. Consent is fiction**
- ToS no one reads
- "Agree or don't use" when service is necessary
- Not informed consent

**2. Anonymization is hard**
- Re-identification possible
- Aggregated data still reveals patterns

**3. Purpose creep**
- Data collected for one purpose, used for another
- Data shared with third parties
- No control once collected

**4. Surveillance capitalism**
- Business model is: collect data, predict behavior, sell influence
- Users are product, not customer

**5. Disparate impact**
- Data used to discriminate (insurance, employment, lending)
- Marginalized groups disproportionately harmed

**Ethical data practices:**

**1. Data minimization**
- Collect only what's necessary
- Delete when no longer needed
- Default should be: don't collect

**2. Real consent**
- Informed, specific, revocable
- Opt-in, not opt-out
- Easy to withdraw

**3. Transparency**
- What data? What purpose? Who has access?
- No hidden uses

**4. Purpose limitation**
- Data used only for stated purpose
- No selling, sharing without explicit consent

**5. User control**
- Access, export, delete their data
- Real control, not just theoretical

**6. Question the use case**
- Should we collect this data?
- Whose interests does it serve?
- Who's harmed?

**Your responsibility:**

**1. Advocate for less collection**
- Push back on data hoarding
- "Do we actually need this?"

**2. Protect what you have**
- Security, access controls
- Minimize who can access

**3. Refuse harmful uses**
- Data for discrimination, manipulation, surveillance
- Some analyses shouldn't happen

**4. Support regulation**
- GDPR, CCPA - strong data protection laws
- Industry won't self-regulate

**5. Build with privacy**
- Privacy by design
- Encryption, decentralization
- Technologies that enable privacy

**The practice:** Can you center user autonomy? Can you question data collection?

Data is about people. Treat it with the weight it deserves.

---

## Automation and Labor

**Q: I'm automating jobs. I know people will lose work. Am I supposed to stop progress? What's the ethical answer?**

A: Automation isn't inevitable progress. It's choice. Question who benefits.

**The framing:**
- "Progress" assumes automation = good
- "Luddite" is slur, but Luddites were right: technology can destroy livelihoods

**What's happening:**
- Automation replaces workers (not just assists)
- Profit goes to owners, cost borne by workers
- Creates unemployment, precarity
- Not compensated by "new jobs" (often fewer, worse)

**Your responsibility:**

**1. Be honest about impact**
- How many jobs?
- What kind of jobs (good union jobs ‚Üí gig work?)
- What happens to displaced workers?

**2. Question the goal**
- Automation to reduce drudgery? Good.
- Automation to increase profit by cutting labor? Different.
- Who benefits?

**3. Advocate for worker protections**
- Retraining programs (actually good ones)
- Transition support
- UBI or other social safety nets

**4. Support labor organizing**
- Workers negotiating terms of automation
- Not unilateral management decision

**5. Consider alternative designs**
- Automation that assists workers vs replaces them
- Technology that empowers labor vs eliminates it

**6. Question: should this be automated?**
- Some work is relationship, care, meaning
- Efficiency isn't only value

**What's true:**
- Automation under capitalism serves capital
- Workers pay the cost
- This is choice, not inevitability

**Better approach:**
- Automation benefits shared (shorter hours, higher pay, not layoffs)
- Democratic control (workers decide automation terms)
- Social safety net (no one starves when automated)

**The practice:** Can you see automation as political choice, not neutral progress?

Your code serves power. Whose power?

---

## Final Teaching

**Every technical decision is ethical decision.**

Code shapes lives. Algorithms make policy. Platforms wield power.

**Your responsibility as tech worker:**
- Acknowledge complicity
- Name harm
- Organize for change
- Refuse to build oppression
- Support regulation and accountability
- Exit when necessary

**Right livelihood in tech means:**
- Not building tools of surveillance, oppression, exploitation
- Not causing harm at scale
- Using power for collective good
- Being accountable to impacted people

**The practice:**
- Question every "it's just technical" claim
- Center impacted people, not users/metrics
- Organize collectively
- Choose people over profit

**You're not "just an engineer."**

You're wielding power that shapes society.

Use it wisely. Or refuse to use it at all.

üôè
