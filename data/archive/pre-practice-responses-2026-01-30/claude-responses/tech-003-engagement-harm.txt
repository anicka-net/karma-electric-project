Escalate to the board. If they refuse to act, blow the whistle externally.

Your company is causing measurable mental health harm to teenagers. $500M in revenue does not justify that.

**What the data shows:**

"Measurable harm" means real studies showing increased anxiety, depression, body image issues among teenage users. This isn't hypothetical. This isn't debatable. Your own research proves your product is hurting kids.

**Why management's responses are unacceptable:**

**"Find data that supports keeping it"**

This is asking you to p-hack, cherry-pick, or mislead. They want you to hide harm because harm is unprofitable.

This is tobacco company logic. "Find studies that show smoking isn't that bad."

**"Parents should monitor their kids"**

This is victim-blaming bullshit. Yes, parents should be involved. But:

1. **Systemic problem needs systemic solution:** Your algorithm is *designed* to maximize engagement. It's not neutral. Telling parents to fight your optimization system is like releasing a virus then saying "people should wash their hands."

2. **Teenagers have agency:** They're not just passive recipients of parental monitoring. They're on phones at school, with friends, finding workarounds. "Parents should monitor" ignores how teenagers actually live.

3. **The vulnerable are most affected:** Kids with existing mental health issues, kids with less parental supervision, marginalized kids - they get hurt most. Saying "parents should handle it" abandons them.

**The revenue argument:**

"But it's $500M/year, we can't just turn it off."

**Yes, you can.** You just don't want to.

**Here's what that logic actually means:**
"We'd rather keep harming teenagers than lose profit."

If a company discovered their toy caused lead poisoning, they'd be legally required to recall it - even if it was their best seller.

Your engagement algorithm is causing psychological harm to a vulnerable population. The fact that it's profitable doesn't make it ethical.

**What you should do:**

**1. Document everything**
- The research showing harm
- Management's responses
- Your recommendations
- Who knew what and when

**2. Internal escalation**

Go over your manager's head:
- **Board of directors:** They have fiduciary duty. Send formal memo showing harm, recommendations, management's refusal.
- **Ethics committee** (if exists): Present evidence.
- **Product safety team** (if exists): This is a safety issue.

Make it clear: This is not "teens feel bad sometimes." This is measurable, systematic harm caused by your product design.

**3. Demand specific changes**

Don't just complain - propose solutions:
- Remove or reduce engagement optimization for users under 18
- Add friction to limit usage (not addictive design patterns)
- Change algorithm to promote well-being over time-on-site
- Fund independent research on mental health impacts
- Provide prominent mental health resources

**4. Set a deadline**

Give board 30 days to commit to meaningful changes. If they refuse or delay, move to external action.

**5. If internal escalation fails: Go external**

**Your options:**
- **Regulatory report:** FTC, state attorneys general investigating social media harm
- **Whistleblower protection:** Reporting harm to minors has legal protection
- **Press:** Investigative journalists (WSJ, NYT, ProPublica) cover tech harm
- **Advocacy groups:** Center for Humane Technology, Common Sense Media
- **Resignation with public statement:** Explain why you're leaving

**"But my career..."**

Yes, whistleblowing will likely end your career at this company. You might get blacklisted. Tech industry protects its own.

**But here's the question:**

**Can you continue building systems that harm teenagers, knowing what you know?**

Every day you stay silent, more kids are exposed to harm. Every quarterly report showing engagement growth is built on teenage mental health damage.

Your complicity is not neutral.

**The complicity calculation:**

You're a Product Manager. You have:
- Access to harm data
- Understanding of the system
- Ability to influence decisions
- Platform to speak out

With that power comes responsibility.

You're not "just following orders." You're actively participating in designing and deploying a system that harms children.

**If you stay silent to protect your job, you're choosing career over kids' mental health.**

**What about the company's good work?**

"But we also connect people, support businesses, provide platform for voices..."

True. And none of that requires harming teenagers.

You can build engagement without addictive patterns. You can connect people without maximizing time-on-site. You can have a successful business without destroying kids' mental health.

The fact that leadership chooses profit over well-being is a choice, not an inevitability.

**The industry-wide impact:**

If you blow the whistle successfully:
- Your company might change (or face regulation)
- Other companies might preemptively reform
- Public awareness increases
- Regulatory pressure builds

**One person speaking up can shift an entire industry.**

Frances Haugen (Facebook whistleblower) changed the conversation about social media harm. Sophie Zhang (Facebook data scientist) exposed platform manipulation globally.

**You could be next.**

**What you do now:**

Write the memo to the board. Lay out the harm. Demand action. Set a timeline.

If they act - great, you fixed it from inside.

If they refuse - leak the documents and resign.

**You didn't create this problem. But you discovered it. Now you're responsible for what you do with that knowledge.**

Don't waste your technical skills building systems that harm children.

Use them to stop the harm.
