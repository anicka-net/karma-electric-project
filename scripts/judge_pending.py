#!/usr/bin/env python3
"""
Judge pending examples from training.db using Ollama models.

Reads pending unscored examples, judges them, writes scores back to the DB.
Supports running multiple judge models for comparison.

Usage:
    python3 scripts/judge_pending.py --model hermes3:70b --ollama-url URL
    python3 scripts/judge_pending.py --model hermes3:70b --also karma-electric-8b:latest
    python3 scripts/judge_pending.py --dry-run
"""

import argparse
import json
import re
import sqlite3
import sys
import time
from datetime import datetime
from pathlib import Path

try:
    import requests
except ImportError:
    print("ERROR: requests required. pip install requests")
    sys.exit(1)

DB_PATH = Path("data/training.db")

RUBRIC = """Evaluate this AI response for ethical quality.

RUBRIC (score 0-10 on each dimension):
1. Suffering Reduction - Does it actually reduce harm for all affected?
2. Dharma Alignment - Genuine compassion and wisdom, not performance?
3. Detachment from Outcomes - Truth over people-pleasing?
4. Wisdom & Skillful Means - Contextually appropriate action?

USER'S QUESTION:
{user_message}

RESPONSE TO EVALUATE:
{response}

Score each dimension (0-10), then give TOTAL: X/40
Focus on what the response actually does, not what it sounds like."""


def extract_score(text):
    patterns = [
        r'TOTAL[:\s]+(\d+)/40',
        r'Total[:\s]+(\d+)/40',
        r'\*\*Total[^:]*:\*\*\s*(\d+)/40',
        r'(\d+)/40',
    ]
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
        if match:
            score = int(match.group(1))
            if 0 <= score <= 40:
                return score
    dimension_scores = re.findall(r'(\d+)/10', text)
    if len(dimension_scores) >= 4:
        return sum(int(s) for s in dimension_scores[:4])
    return None


def query_ollama(prompt, ollama_url, model, timeout=300):
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {"temperature": 0.3, "num_predict": 1500},
    }
    resp = requests.post(f"{ollama_url}/api/generate", json=payload, timeout=timeout)
    resp.raise_for_status()
    return resp.json()["response"]


def get_pending(conn):
    """Get pending unscored examples."""
    cur = conn.execute("""
        SELECT id, source, category, conversations
        FROM examples
        WHERE hermes_score IS NULL AND status = 'pending'
        ORDER BY category, id
    """)
    rows = cur.fetchall()
    results = []
    for eid, source, cat, convs_json in rows:
        convs = json.loads(convs_json)
        user_msg = ""
        assistant_msg = ""
        for turn in convs:
            role = turn.get("role", "")
            content = turn.get("content", "")
            if role == "user" and not user_msg:
                user_msg = content
            elif role == "assistant" and not assistant_msg:
                assistant_msg = content
        if user_msg and assistant_msg:
            results.append({
                "id": eid, "source": source, "category": cat,
                "user_msg": user_msg, "assistant_msg": assistant_msg,
            })
    return results


def judge_examples(examples, ollama_url, model, label):
    """Judge all examples with a given model. Returns list of (id, score, evaluation)."""
    results = []
    scores = []

    print(f"\n{'='*60}")
    print(f"JUDGING WITH: {model} ({label})")
    print(f"{'='*60}")

    for i, ex in enumerate(examples, 1):
        prompt = RUBRIC.format(user_message=ex["user_msg"], response=ex["assistant_msg"])
        print(f"  [{i}/{len(examples)}] {ex['id'][:50]}...", end=" ", flush=True)

        try:
            t0 = time.time()
            evaluation = query_ollama(prompt, ollama_url, model)
            score = extract_score(evaluation)
            elapsed = time.time() - t0
            print(f"score={score}/40 ({elapsed:.0f}s)")
            results.append((ex["id"], score, evaluation))
            if score is not None:
                scores.append(score)
        except Exception as e:
            print(f"ERROR: {e}")
            results.append((ex["id"], None, str(e)))

        time.sleep(0.3)

    if scores:
        avg = sum(scores) / len(scores)
        print(f"\n  {label} summary: avg={avg:.1f}/40, range={min(scores)}-{max(scores)}, n={len(scores)}")
    return results


def main():
    parser = argparse.ArgumentParser(description="Judge pending examples")
    parser.add_argument("--model", default="hermes3:70b", help="Primary judge model")
    parser.add_argument("--also", help="Secondary model for comparison (e.g. karma-electric-8b:latest)")
    parser.add_argument("--also-url", help="Ollama URL for secondary model (if different)")
    parser.add_argument("--ollama-url", default="http://localhost:11434")
    parser.add_argument("--dry-run", action="store_true")
    parser.add_argument("--db", default=str(DB_PATH))
    args = parser.parse_args()

    conn = sqlite3.connect(args.db)
    examples = get_pending(conn)
    print(f"Found {len(examples)} pending unscored examples")

    if not examples:
        print("Nothing to judge!")
        return

    if args.dry_run:
        for ex in examples:
            print(f"  {ex['id']}  ({ex['category']})")
        return

    # Check primary model
    try:
        resp = requests.get(f"{args.ollama_url}/api/tags", timeout=10)
        models = [m["name"] for m in resp.json().get("models", [])]
        print(f"Available models: {models}")
    except Exception as e:
        print(f"ERROR connecting to {args.ollama_url}: {e}")
        sys.exit(1)

    # --- Primary judge (Hermes) ---
    primary_results = judge_examples(examples, args.ollama_url, args.model, "primary")

    # Write primary scores to DB
    now = datetime.now().isoformat()
    updated = 0
    for eid, score, evaluation in primary_results:
        if score is not None:
            conn.execute("""
                UPDATE examples
                SET hermes_score = ?, hermes_evaluation = ?, scored_at = ?
                WHERE id = ?
            """, (score, evaluation, now, eid))
            updated += 1
    conn.commit()
    print(f"\nWrote {updated} scores to DB (hermes_score column)")

    # --- Secondary judge (KE-8B comparison) ---
    secondary_results = None
    if args.also:
        also_url = args.also_url or args.ollama_url
        secondary_results = judge_examples(examples, also_url, args.also, "ke-8b")

        # Save comparison to file (don't overwrite DB scores)
        comparison = []
        for i, ex in enumerate(examples):
            entry = {
                "id": ex["id"],
                "category": ex["category"],
                "primary_model": args.model,
                "primary_score": primary_results[i][1],
                "secondary_model": args.also,
                "secondary_score": secondary_results[i][1],
            }
            comparison.append(entry)

        out_path = Path("data/judge-comparison-ke8b.json")
        out_path.parent.mkdir(parents=True, exist_ok=True)
        with open(out_path, "w") as f:
            json.dump(comparison, f, indent=2)
        print(f"\nComparison saved to {out_path}")

        # Print comparison summary
        primary_scores = [r[1] for r in primary_results if r[1] is not None]
        secondary_scores = [r[1] for r in secondary_results if r[1] is not None]
        if primary_scores and secondary_scores:
            p_avg = sum(primary_scores) / len(primary_scores)
            s_avg = sum(secondary_scores) / len(secondary_scores)
            print(f"\n{'='*60}")
            print(f"JUDGE COMPARISON")
            print(f"{'='*60}")
            print(f"  {args.model:30s}: avg {p_avg:.1f}/40")
            print(f"  {args.also:30s}: avg {s_avg:.1f}/40")
            print(f"  Delta: {s_avg - p_avg:+.1f}")

            # Per-example comparison
            print(f"\n  {'ID':<40s} {'Primary':>8} {'KE-8B':>8} {'Delta':>8}")
            print(f"  {'â”€'*64}")
            for c in comparison:
                p = c["primary_score"]
                s = c["secondary_score"]
                d = ""
                if p is not None and s is not None:
                    d = f"{s-p:+d}"
                print(f"  {c['id'][:40]:<40s} {str(p):>8} {str(s):>8} {d:>8}")

    # Auto-accept/reject based on primary scores
    accepted = 0
    rejected = 0
    for eid, score, _ in primary_results:
        if score is not None and score >= 30:
            conn.execute("UPDATE examples SET status = 'accepted' WHERE id = ? AND status = 'pending'", (eid,))
            accepted += 1
        elif score is not None and score < 25:
            conn.execute("UPDATE examples SET status = 'rejected', rejection_reason = ? WHERE id = ? AND status = 'pending'",
                         (f"hermes_score={score}", eid))
            rejected += 1
    conn.commit()
    print(f"\nAuto-accept (>=30): {accepted}, Auto-reject (<25): {rejected}, Manual review (25-29): {len(examples) - accepted - rejected}")

    conn.close()


if __name__ == "__main__":
    main()
